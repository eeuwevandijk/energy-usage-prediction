{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>EnergyUsagePrediction.model_def.asum.v1_0_7.ipynb</b>\n",
    "<br/>For my use case \"Energy usage prediction based on historical weather and energy usage data.\". The original dataset  can be downloaded from <a href=\"https://www.kaggle.com/taranvee/smart-home-dataset-with-weather-information\">kaggle</a>\n",
    "<br/>The dataset used in this step (feature engineering) has already been transformed in the ETL step.\n",
    "<br/>Data exploration is described/performed in \"EnergyUsagePrediction.data_exp.asum.1_0_5.Ipynb\"\n",
    "<br/>ETL is described/performed in \"EnergyUsagePrediction.etl.asum.1_0_8.Ipynb\"\n",
    "<br/>Feature engineering is described/performed in \"EnergyUsagePrediction.feature_eng.asum.1_0_8.Ipynb\"\n",
    "<br/>\n",
    "<br/>This task defines the machine learning or deep learning model.\n",
    "<br/>\n",
    "<br/>Load <i>smart-home-dataset-with-weather-information_post_feature_eng.csv</i> file into pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_x = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='[credentials]',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_x.get_object(Bucket='default-donotdelete-pr-dczw8ajohz6wjh',Key='smart-home-dataset-with-weather-information_post_feature_eng.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df = pd.read_csv(body)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "#from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For usability we define constants for the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbTimestamp = 'Timestamp'\n",
    "lbTotalEneryUsage = 'TotalUsage_kW'\n",
    "lbTemperature = 'Temperature_F'\n",
    "lbTemperatureNormalized = 'Temperature_F_normalized'\n",
    "lbHumidity = 'Humidity'\n",
    "lbHumidityNormalized = 'Humidity_normalized'\n",
    "lbPressure = 'Pressure_hPa'\n",
    "lbPressureNormalized = 'Pressure_hPa_normalized'\n",
    "lbWindSpeed = 'WindSpeed'\n",
    "lbWindSpeedNormalized = 'WindSpeed_normalized'\n",
    "lbCloudCover = 'cloudCover'\n",
    "lbCloudCoverNormalized = 'cloudCover_normalized'\n",
    "lbWindBearing = 'WindBearing'\n",
    "lbWindBearingNormalized = 'WindBearing_normalized'\n",
    "lbPrecipIntensity = 'PrecipIntensity'\n",
    "lbPrecipIntensityNormalized = 'PrecipIntensity_normalized'\n",
    "lbDewPoint = 'dewPoint_F'\n",
    "lbDewPointNormalized = 'dewPoint_F_normalized'\n",
    "lbDayOfYear='dayOfYear'\n",
    "lbDayOfYearNormalized='dayOfYear_normalized'\n",
    "lbHourOfDay='hourOfDay'\n",
    "lbHourOfDayNormalized='hourOfDay_normalized'\n",
    "lbMinuteOfDay='minuteOfDay'\n",
    "lbMinuteOfDayNormalized='minuteOfDay_normalized'\n",
    "\n",
    "lbWeatherIndicatorClearDay = 'weatherIndicator_clear-day'\n",
    "lbWeatherIndicatorClearNight = 'weatherIndicator_clear-night'\n",
    "lbWeatherIndicatorCloudy = 'weatherIndicator_cloudy'\n",
    "lbWeatherIndicatorFog = 'weatherIndicator_fog'\n",
    "lbWeatherIndicatorPartlyCloudyDay = 'weatherIndicator_partly-cloudy-day'\n",
    "lbWeatherIndicatorPartlyCloudyNight = 'weatherIndicator_partly-cloudy-night'\n",
    "lbWeatherIndicatorRain = 'weatherIndicator_rain'\n",
    "lbWeatherIndicatorSnow = 'weatherIndicator_snow'\n",
    "lbWeatherIndicatorWind = 'weatherIndicator_wind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take 80% of the dataset for training, 20 procent for testing.\n",
    "<br/>We use a seed to build deterministic training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputColumns = [lbTemperatureNormalized,\n",
    "                lbHumidityNormalized,\n",
    "                lbWindSpeedNormalized,\n",
    "                lbWindBearingNormalized,\n",
    "                lbDewPointNormalized,\n",
    "                lbPrecipIntensityNormalized, \n",
    "                lbDayOfYearNormalized,\n",
    "                lbHourOfDayNormalized, \n",
    "                lbMinuteOfDayNormalized, \n",
    "                lbWeatherIndicatorClearDay,\n",
    "                lbWeatherIndicatorClearNight,\n",
    "                lbWeatherIndicatorCloudy,\n",
    "                lbWeatherIndicatorFog,\n",
    "                lbWeatherIndicatorPartlyCloudyDay,\n",
    "                lbWeatherIndicatorPartlyCloudyNight,\n",
    "                lbWeatherIndicatorRain,\n",
    "                lbWeatherIndicatorSnow,\n",
    "                lbWeatherIndicatorWind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputColumns = [lbTotalEneryUsage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
    "test=df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train[inputColumns]\n",
    "test_x=test[inputColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=train[outputColumns]\n",
    "test_y=test[outputColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use a traditional machine learning algorithm: LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sklearn\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = regr.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_y, predicted), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(test_y, predicted), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(test_y, predicted), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(test_y, predicted), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(test_y, predicted), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test.sample(200,random_state=42)\n",
    "samples_x=samples[inputColumns]\n",
    "samples_y=samples[outputColumns]\n",
    "predictedSamples = regr.predict(samples_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure=plt.figure(figsize=(12,12))\n",
    "samples_y = samples.reset_index()\n",
    "plt.plot(samples_y[lbTotalEneryUsage], figure=figure)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"actual+predicted\")\n",
    "plt.plot(predictedSamples, figure=figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now  let's start with a Deep Learning approach using Keras Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras model\n",
    "model = Sequential()\n",
    "batch_size = 32\n",
    "input_dim=18\n",
    "model.add(Dense(batch_size*input_dim, kernel_initializer = \"uniform\",input_dim=input_dim, name=\"input\"))\n",
    "model.add(Dense(256, activation=\"relu\", name=\"hiddenlayer1\"))\n",
    "model.add(Dense(1, name=\"output\"))\n",
    "\n",
    "# Gradient descent algorithm\n",
    "adam = Adam(0.001)\n",
    "\n",
    "model.compile(loss='mse', optimizer=adam)\n",
    "history = model.fit(train_x, train_y, epochs=15, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"No. of Iterations\")\n",
    "plt.ylabel(\"J(Theta1 Theta0)/Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_y, predicted), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(test_y, predicted), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(test_y, predicted), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(test_y, predicted), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(test_y, predicted), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSamplePredictedVsActual(myModel, testData):\n",
    "    test_sample= testData.sample(200,random_state=42)\n",
    "    test_sample = test_sample.reset_index()\n",
    "    test_sample_x = test_sample[inputColumns]\n",
    "    test_sample_y = test_sample[outputColumns]\n",
    "    test_predicted = myModel.predict(test_sample_x)\n",
    "    figure=plt.figure(figsize=(12,12))\n",
    "    plt.plot(test_sample[outputColumns], figure=figure)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"actual\")\n",
    "    plt.plot(test_predicted, figure=figure)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSamplePredictedVsActual(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(predicted.min()) +','+str(predicted.max())+str(predicted.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras model2\n",
    "model2 = Sequential()\n",
    "batch_size = 32\n",
    "input_dim=18\n",
    "model2.add(Dense(batch_size*input_dim, kernel_initializer = \"uniform\",input_dim=input_dim, name=\"input\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(256, activation=\"relu\", name=\"hiddenlayer1\"))\n",
    "model2.add(Dense(256, activation=\"relu\", name=\"hiddenlayer2\"))\n",
    "model2.add(Dense(256, activation=\"relu\", name=\"hiddenlayer3\"))\n",
    "model2.add(Dense(256, activation=\"relu\", name=\"hiddenlayer4\"))\n",
    "model2.add(Dense(256, activation=\"relu\", name=\"hiddenlayer5\"))\n",
    "model2.add(Dense(1, name=\"output\"))\n",
    "\n",
    "# Gradient descent algorithm\n",
    "#adam = Adam(0.1)\n",
    "adam = Adam(0.00001)\n",
    "\n",
    "model2.compile(loss='mse', optimizer=adam)\n",
    "history = model2.fit(train_x, train_y, epochs=25, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"No. of Iterations\")\n",
    "plt.ylabel(\"J(Theta1 Theta0)/Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x=test[inputColumns]\n",
    "test_y=test[outputColumns]\n",
    "model2.evaluate(test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model2.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_y, predicted), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(test_y, predicted), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(test_y, predicted), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(test_y, predicted), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(test_y, predicted), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSamplePredictedVsActual(model2, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the Dense Neural network performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
